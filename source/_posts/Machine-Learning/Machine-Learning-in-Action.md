---
title: Machine-Learning-in-Action
date: 2017-12-05 07:12:56
categories: Machine-Learning
tags:
---
第一部分 分类
第1章 机器学习基础

第2章 k-近邻算法

第3章 决策树


决策树要合理的选择决策顺序, 减少树的高度, 如何选择分枝



信息墒entropy

我们用信息量来描述 数据的不确定性，信息墒就是信息量的期望。信息墒越高，说明信息量的期望越大，数据越不确定。
说白来墒越大就越混乱。

比如“明天太阳从东方出来” 是确定事件，墒为0

另一个度量集合无序程度的方法是基尼不纯度(Gini impurity) , 简单的说就是从一个数据集中随机选取子项，度量其被错误分类到其他组里 的概率

组合分类器关键：

从原始数据集抽样出K子学习集，怎么做抽样

数据被抽样出来后，形成新的学习集,新的学习集怎么训练出新的分类器

bagging: 装袋

1) 有放回抽样. 前后两次抽样，条件独立

bootstrap: 
2) 自主样本,从原始数据集里衍生出新的数据集 (有放回)
优势
准确率明显高于组合中任何单个的分类器

boosting 算法:
准确率特别高.
训练集中的元组被分配权重
权重影响抽样，权重越大，越可能被抽取
迭代训练若干个分类器，在前一个分类器中被错误分类的元组。

可能过拟合

Adaboost,给每一个加权投票

随机森林(Random Forest) 算法
决策树加强版，
由很多决策树分类器组合而成(因而称为“森林”)
单个的决策树分类器用随机方法构成。 首先，学习

剪枝是为了防止过度拟合，但是现在因为组合，防止了过拟合

优点
准备率可以和Adaboost媲美
对错误和离群点更加鲁棒性
决策树容易

第4章 基于概率论的分类方法:朴素贝叶斯

第5章 Logistic 回归

第6章 支持向量机

第7章 利用AdaBoost 元算法提高分类性能

第二部分 利用回归预测数值型数据
第8章 预测数值性数据：回归

第9章 树回归

当数据拥有众多特征并且特征关系之间关系之分复杂时，构建全局模型的想法就显得太难了。

线性回归不足以解决实际生活中很多非线性的问题。

一种可行的方法是将数据集切分成很多份易建模的数据，然后用线性回归技术来建模。

如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树结构和回归法就相当有用。

CART (Classsification And Regression Trees) 算法， 该算法既可以用于分类，还可以用于回归。

回归与模型树, 

树剪枝算法 防止树的过拟合

9.1 复杂数据的局部性建模




### 9.1 复杂数据的局部性建模
第3章里ID3构建树的算法，是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。如果一个特征有四种取值，那么数据将被切成4份。一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用。

除了切分过于迅速外，ID3 另一个问题就是不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在ID3中使用。

CART 使用二元切分来处理连续型变量。对CART 稍作修改就可以处理回归问题。

树回归的一般方法

(1) 收集数据：采用任意方法收集数据。
(2) 准备数据： 需要数值型的数据，标称数据应该映射成二值数据。
(3) 分析数据: 绘出数据的二维可视化显示结果，以字典方式生成树。
(4) 训练算法: 大部分时间都花费在叶节点树模型的构建上。
(5) 测试算法：使用测试数据上的R2值来分析建模的效果。
(6) 使用算法: 使用训练出的树做预测，预测结果还可以用来做很多事情。

### 9.2 连续和离散型特征的树的构建


### 9.3 将CART 算法用于回归
#### 9.3.1 构建树
####  9.3.2 运行代码
### 9.4 树剪枝
#### 9.4.1 预剪枝
#### 9.4.2 后剪枝
### 9.5 模型树
  

第三部分 无监督学习

第10章 利用k-均值聚类算法对为标注数据分类

第11章 使用Apriori 算法进行关联分析

第12章 使用FP-growth 算法来高效发现频繁项集

第四部分 其他工具

第13章 利用PCA来简化数据

第14章 利用SVD 简化数据

第15章 大数据与MapReduce



