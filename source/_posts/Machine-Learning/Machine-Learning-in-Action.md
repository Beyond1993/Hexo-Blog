---
title: Machine-Learning-in-Action
date: 2017-12-05 07:12:56
categories: Machine-Learning
tags:
---
第一部分 分类
第1章 机器学习基础

第2章 k-近邻算法

第3章 决策树


决策树要合理的选择决策顺序, 减少树的高度, 如何选择分枝



信息墒entropy

我们用信息量来描述 数据的不确定性，信息墒就是信息量的期望。信息墒越高，说明信息量的期望越大，数据越不确定。
说白来墒越大就越混乱。

比如“明天太阳从东方出来” 是确定事件，墒为0

另一个度量集合无序程度的方法是基尼不纯度(Gini impurity) , 简单的说就是从一个数据集中随机选取子项，度量其被错误分类到其他组里 的概率

组合分类器关键：

从原始数据集抽样出K子学习集，怎么做抽样

数据被抽样出来后，形成新的学习集,新的学习集怎么训练出新的分类器

bagging: 装袋

1) 有放回抽样. 前后两次抽样，条件独立

bootstrap: 
2) 自主样本,从原始数据集里衍生出新的数据集 (有放回)
优势
准确率明显高于组合中任何单个的分类器

boosting 算法:
准确率特别高.
训练集中的元组被分配权重
权重影响抽样，权重越大，越可能被抽取
迭代训练若干个分类器，在前一个分类器中被错误分类的元组。

可能过拟合

Adaboost,给每一个加权投票

随机森林(Random Forest) 算法
决策树加强版，
由很多决策树分类器组合而成(因而称为“森林”)
单个的决策树分类器用随机方法构成。 首先，学习

剪枝是为了防止过度拟合，但是现在因为组合，防止了过拟合

优点
准备率可以和Adaboost媲美
对错误和离群点更加鲁棒性
决策树容易

第4章 基于概率论的分类方法:朴素贝叶斯

第5章 Logistic 回归

第6章 支持向量机

第7章 利用AdaBoost 元算法提高分类性能

第二部分 利用回归预测数值型数据
第8章 预测数值性数据：回归
第9章 树回归

第三部分 无监督学习

第10章 利用k-均值聚类算法对为标注数据分类

第11章 使用Apriori 算法进行关联分析

第12章 使用FP-growth 算法来高效发现频繁项集

第四部分 其他工具

第13章 利用PCA来简化数据

第14章 利用SVD 简化数据

第15章 大数据与MapReduce



